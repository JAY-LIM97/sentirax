"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

ğŸ¯ ëª©ì :
ê³¼ì í•© ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°

ğŸ“Š ì‹œë„í•  ì¡°í•©:
- max_depth: 3, 5, 7 (í˜„ì¬ 10ì—ì„œ ì¤„ì´ê¸°)
- min_samples_split: 10, 15, 20 (í˜„ì¬ 5ì—ì„œ ëŠ˜ë¦¬ê¸°)
- n_estimators: 50, 100 (íŠ¸ë¦¬ ê°œìˆ˜)
"""

import sys
import os
import io
import platform

# Windows í•œê¸€/ì´ëª¨ì§€ ì¶œë ¥ ì„¤ì •
if platform.system() == 'Windows':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from itertools import product

from core.feature_engineer import FeatureEngineer
from core.ml_trainer import MLTrainer


def main():
    print("=" * 70)
    print("ğŸ”§ Step 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("=" * 70)

    # ========================================
    # 1. ë°ì´í„° ì¤€ë¹„
    # ========================================
    print("\nğŸ“ ë°ì´í„° ë¡œë”© ë° Feature Engineering...")

    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'tsla_optimized_90days.csv')
    df = pd.read_csv(data_path, index_col=0)

    engineer = FeatureEngineer(label_threshold=1.0)
    X, y = engineer.prepare_ml_data(df)

    feature_names = X.columns.tolist()

    # ========================================
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
    # ========================================
    print("\nâš™ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •...")

    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [3, 5, 7],
        'min_samples_split': [10, 15, 20],
        'min_samples_leaf': [5, 7, 10]
    }

    print(f"  - n_estimators: {param_grid['n_estimators']}")
    print(f"  - max_depth: {param_grid['max_depth']}")
    print(f"  - min_samples_split: {param_grid['min_samples_split']}")
    print(f"  - min_samples_leaf: {param_grid['min_samples_leaf']}")

    # ì „ì²´ ì¡°í•© ìˆ˜
    total_combinations = (len(param_grid['n_estimators']) *
                         len(param_grid['max_depth']) *
                         len(param_grid['min_samples_split']) *
                         len(param_grid['min_samples_leaf']))

    print(f"\n  ì´ {total_combinations}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸")

    # ========================================
    # 3. ê·¸ë¦¬ë“œ ì„œì¹˜
    # ========================================
    print("\nğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘...\n")

    results = []
    best_score = 0
    best_params = None

    for i, (n_est, max_d, min_split, min_leaf) in enumerate(
        product(
            param_grid['n_estimators'],
            param_grid['max_depth'],
            param_grid['min_samples_split'],
            param_grid['min_samples_leaf']
        ), 1
    ):
        print(f"[{i}/{total_combinations}] í…ŒìŠ¤íŠ¸ ì¤‘: n_est={n_est}, max_depth={max_d}, "
              f"min_split={min_split}, min_leaf={min_leaf}")

        # Trainer ì´ˆê¸°í™”
        trainer = MLTrainer(test_size=0.2, random_state=42)
        trainer.feature_names = feature_names

        # Train/Test ë¶„í• 
        X_train, X_test, y_train, y_test = trainer.split_data(X, y)

        # ì •ê·œí™”
        X_train_scaled, X_test_scaled = trainer.normalize_data(X_train, X_test)

        # í•™ìŠµ
        try:
            trainer.train_random_forest(
                X_train_scaled,
                y_train,
                n_estimators=n_est,
                max_depth=max_d,
                min_samples_split=min_split,
                min_samples_leaf=min_leaf
            )

            # í‰ê°€
            eval_results = trainer.evaluate_model(
                X_test_scaled,
                y_test,
                X_train_scaled,
                y_train
            )

            # ê³¼ì í•© ì°¨ì´
            overfit_gap = eval_results['train_accuracy'] - eval_results['test_accuracy']

            # ê²°ê³¼ ì €ì¥
            result = {
                'n_estimators': n_est,
                'max_depth': max_d,
                'min_samples_split': min_split,
                'min_samples_leaf': min_leaf,
                'train_accuracy': eval_results['train_accuracy'],
                'test_accuracy': eval_results['test_accuracy'],
                'test_precision': eval_results['test_precision'],
                'test_recall': eval_results['test_recall'],
                'test_f1': eval_results['test_f1'],
                'overfit_gap': overfit_gap
            }
            results.append(result)

            print(f"  â†’ Test Acc: {eval_results['test_accuracy']*100:.1f}%, "
                  f"Overfit Gap: {overfit_gap*100:.1f}%p, "
                  f"F1: {eval_results['test_f1']:.3f}")

            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (F1-Score ê¸°ì¤€, ê³¼ì í•© ê³ ë ¤)
            # F1-Scoreë¥¼ ì£¼ìš” ì§€í‘œë¡œ, ê³¼ì í•© ì°¨ì´ê°€ 30% ë¯¸ë§Œì¸ ê²½ìš°ë§Œ
            if eval_results['test_f1'] > best_score and overfit_gap < 0.3:
                best_score = eval_results['test_f1']
                best_params = result.copy()

        except Exception as e:
            print(f"  â†’ ì˜¤ë¥˜ ë°œìƒ: {e}")

        print()

    # ========================================
    # 4. ê²°ê³¼ ë¶„ì„
    # ========================================
    print("=" * 70)
    print("ğŸ“Š ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼")
    print("=" * 70)

    results_df = pd.DataFrame(results)

    # ê²°ê³¼ ì •ë ¬ (Test F1-Score ê¸°ì¤€)
    results_df = results_df.sort_values('test_f1', ascending=False)

    print(f"\nğŸ† ìƒìœ„ 5ê°œ ì¡°í•© (F1-Score ê¸°ì¤€):\n")
    print(results_df.head(5).to_string(index=False))

    # ê³¼ì í•©ì´ ì ì€ ì¡°í•©
    print(f"\nâœ… ê³¼ì í•©ì´ ì ì€ ìƒìœ„ 5ê°œ (Overfit Gap < 30%):\n")
    low_overfit = results_df[results_df['overfit_gap'] < 0.3].head(5)
    if len(low_overfit) > 0:
        print(low_overfit.to_string(index=False))
    else:
        print("  ê³¼ì í•©ì´ ì ì€ ì¡°í•©ì´ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ 30% ì´ìƒ)")

    # ìµœì  íŒŒë¼ë¯¸í„°
    if best_params:
        print(f"\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (F1-Score ìµœëŒ€ + Overfit < 30%):")
        print(f"  - n_estimators: {best_params['n_estimators']}")
        print(f"  - max_depth: {best_params['max_depth']}")
        print(f"  - min_samples_split: {best_params['min_samples_split']}")
        print(f"  - min_samples_leaf: {best_params['min_samples_leaf']}")
        print(f"\n  ì„±ëŠ¥:")
        print(f"  - Test Accuracy: {best_params['test_accuracy']*100:.2f}%")
        print(f"  - Test F1-Score: {best_params['test_f1']:.3f}")
        print(f"  - Overfit Gap: {best_params['overfit_gap']*100:.1f}%p")
    else:
        print(f"\nâš ï¸  ê³¼ì í•©ì´ ì ì€(30% ë¯¸ë§Œ) ì¡°í•©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        print(f"  â†’ ê³¼ì í•©ì´ ê°€ì¥ ì ì€ ì¡°í•© ì„ íƒ")
        best_params = results_df.sort_values('overfit_gap').iloc[0].to_dict()
        print(f"\n  ì„ íƒëœ íŒŒë¼ë¯¸í„°:")
        print(f"  - n_estimators: {int(best_params['n_estimators'])}")
        print(f"  - max_depth: {int(best_params['max_depth'])}")
        print(f"  - min_samples_split: {int(best_params['min_samples_split'])}")
        print(f"  - min_samples_leaf: {int(best_params['min_samples_leaf'])}")

    # ========================================
    # 5. ê²°ê³¼ ì €ì¥
    # ========================================
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥...")

    results_dir = os.path.join(os.path.dirname(__file__), '..', 'results')
    results_path = os.path.join(results_dir, 'hyperparameter_tuning_results.csv')
    results_df.to_csv(results_path, index=False)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {results_path}")

    # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
    best_params_path = os.path.join(results_dir, 'best_hyperparameters.csv')
    pd.DataFrame([best_params]).to_csv(best_params_path, index=False)
    print(f"âœ… ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: {best_params_path}")

    # ========================================
    # 6. ì¸ì‚¬ì´íŠ¸
    # ========================================
    print(f"\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")

    # max_depth ë¶„ì„
    avg_by_depth = results_df.groupby('max_depth').agg({
        'test_accuracy': 'mean',
        'overfit_gap': 'mean'
    })
    print(f"\n  ğŸ“Š max_depthë³„ í‰ê· :")
    for depth, row in avg_by_depth.iterrows():
        print(f"    depth={depth}: Test Acc={row['test_accuracy']*100:.1f}%, "
              f"Overfit={row['overfit_gap']*100:.1f}%p")

    print("\n" + "=" * 70)
    print("âœ¨ Step 1 ì™„ë£Œ!")
    print("=" * 70)


if __name__ == "__main__":
    main()
