"""
ì²« ë²ˆì§¸ ML ëª¨ë¸ í•™ìŠµ: Random Forest

ğŸ¯ ëª©ì :
1. Feature Engineeringìœ¼ë¡œ ìƒì„±í•œ ë°ì´í„°ë¡œ ì²« ML ëª¨ë¸ í•™ìŠµ
2. Random Forestë¡œ ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸ ì˜ˆì¸¡
3. ì„±ëŠ¥ í‰ê°€ ë° Feature Importance ë¶„ì„
4. ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥

ğŸ“Š ì „ì²´ íë¦„:
ë°ì´í„° ë¡œë“œ â†’ ì •ê·œí™” â†’ Train/Test ë¶„í•  â†’ í•™ìŠµ â†’ í‰ê°€ â†’ ì‹œê°í™”
"""

import sys
import os
import io
import platform

# Windows í•œê¸€/ì´ëª¨ì§€ ì¶œë ¥ ì„¤ì •
if platform.system() == 'Windows':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

from core.feature_engineer import FeatureEngineer
from core.ml_trainer import MLTrainer


def plot_feature_importance(feature_imp: pd.DataFrame, top_n: int = 15, save_path: str = None):
    """
    Feature Importance ì‹œê°í™”

    Args:
        feature_imp: Feature Importance DataFrame
        top_n: ìƒìœ„ Nê°œ íŠ¹ì§•ë§Œ í‘œì‹œ
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í‘œì‹œë§Œ)
    """

    plt.figure(figsize=(10, 8))

    # ìƒìœ„ Nê°œë§Œ ì„ íƒ
    top_features = feature_imp.head(top_n)

    # ë§‰ëŒ€ ê·¸ë˜í”„
    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
    bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)

    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance Score', fontsize=12)
    plt.title(f'Top {top_n} Feature Importance (Random Forest)', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()  # ì¤‘ìš”ë„ ë†’ì€ ê²ƒì´ ìœ„ë¡œ

    # ê°’ í‘œì‹œ
    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width, bar.get_y() + bar.get_height()/2,
                f'{width:.4f}',
                ha='left', va='center', fontsize=9)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Feature Importance ì°¨íŠ¸ ì €ì¥: {save_path}")

    plt.close()


def plot_confusion_matrix(cm: np.ndarray, save_path: str = None):
    """
    Confusion Matrix ì‹œê°í™”

    Args:
        cm: Confusion Matrix
        save_path: ì €ì¥ ê²½ë¡œ
    """

    plt.figure(figsize=(8, 6))

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
                xticklabels=['ì˜ˆì¸¡ ë§¤ë„(0)', 'ì˜ˆì¸¡ ë§¤ìˆ˜(1)'],
                yticklabels=['ì‹¤ì œ ë§¤ë„(0)', 'ì‹¤ì œ ë§¤ìˆ˜(1)'])

    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('Actual', fontsize=12)
    plt.xlabel('Predicted', fontsize=12)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Confusion Matrix ì°¨íŠ¸ ì €ì¥: {save_path}")

    plt.close()


def main():
    print("=" * 70)
    print("ğŸš€ Day 2: ì²« ë²ˆì§¸ ML ëª¨ë¸ í•™ìŠµ (Random Forest)")
    print("=" * 70)

    # ========================================
    # 1. ë°ì´í„° ë¡œë“œ
    # ========================================
    print("\nğŸ“ [Step 1] ë°ì´í„° ë¡œë”©...")

    data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'tsla_optimized_90days.csv')
    df = pd.read_csv(data_path, index_col=0)

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")

    # ========================================
    # 2. Feature Engineering
    # ========================================
    print("\nğŸ”§ [Step 2] Feature Engineering...")

    engineer = FeatureEngineer(label_threshold=1.0)
    X, y = engineer.prepare_ml_data(df)

    print(f"âœ… Feature Engineering ì™„ë£Œ")
    print(f"  - íŠ¹ì§• ìˆ˜: {X.shape[1]}ê°œ")
    print(f"  - í•™ìŠµ ë°ì´í„°: {len(X)}ê°œ")

    # íŠ¹ì§• ì´ë¦„ ì €ì¥
    feature_names = X.columns.tolist()

    # ========================================
    # 3. ML Trainer ì´ˆê¸°í™” ë° ë°ì´í„° ë¶„í• 
    # ========================================
    print("\nâš™ï¸  [Step 3] ML Trainer ì´ˆê¸°í™”...")

    trainer = MLTrainer(test_size=0.2, random_state=42)
    trainer.feature_names = feature_names

    # Train/Test ë¶„í• 
    X_train, X_test, y_train, y_test = trainer.split_data(X, y)

    # ========================================
    # 4. ë°ì´í„° ì •ê·œí™”
    # ========================================
    X_train_scaled, X_test_scaled = trainer.normalize_data(X_train, X_test)

    # ========================================
    # 5. Random Forest í•™ìŠµ
    # ========================================
    model = trainer.train_random_forest(
        X_train_scaled,
        y_train,
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2
    )

    # ========================================
    # 6. ì„±ëŠ¥ í‰ê°€
    # ========================================
    results = trainer.evaluate_model(
        X_test_scaled,
        y_test,
        X_train_scaled,
        y_train
    )

    # ========================================
    # 7. Feature Importance ë¶„ì„
    # ========================================
    print("\nğŸ” [Step 5] Feature Importance ë¶„ì„...")

    feature_imp = trainer.get_feature_importance(feature_names)

    print(f"\nğŸ“Š ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•:")
    for i, row in feature_imp.head(10).iterrows():
        print(f"  {i+1:2d}. {row['feature']:20s}: {row['importance']:.4f}")

    # ========================================
    # 8. ê²°ê³¼ ì‹œê°í™”
    # ========================================
    print("\nğŸ“ˆ [Step 6] ê²°ê³¼ ì‹œê°í™”...")

    # results í´ë” ìƒì„±
    results_dir = os.path.join(os.path.dirname(__file__), '..', 'results')
    os.makedirs(results_dir, exist_ok=True)

    # Feature Importance ì°¨íŠ¸
    fi_path = os.path.join(results_dir, 'feature_importance_rf.png')
    plot_feature_importance(feature_imp, top_n=15, save_path=fi_path)

    # Confusion Matrix ì°¨íŠ¸
    cm_path = os.path.join(results_dir, 'confusion_matrix_rf.png')
    plot_confusion_matrix(results['confusion_matrix'], save_path=cm_path)

    # ========================================
    # 9. ê²°ê³¼ ì €ì¥
    # ========================================
    print("\nğŸ’¾ [Step 7] ê²°ê³¼ ì €ì¥...")

    # Feature Importance CSV
    fi_csv_path = os.path.join(results_dir, 'feature_importance_rf.csv')
    feature_imp.to_csv(fi_csv_path, index=False)
    print(f"âœ… Feature Importance ì €ì¥: {fi_csv_path}")

    # ì„±ëŠ¥ ì§€í‘œ ì €ì¥
    results_summary = {
        'model': 'Random Forest',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'train_size': len(X_train),
        'test_size': len(X_test),
        'test_accuracy': results['test_accuracy'],
        'test_precision': results['test_precision'],
        'test_recall': results['test_recall'],
        'test_f1': results['test_f1'],
        'train_accuracy': results.get('train_accuracy', None)
    }

    results_df = pd.DataFrame([results_summary])
    results_csv_path = os.path.join(results_dir, 'model_performance_rf.csv')
    results_df.to_csv(results_csv_path, index=False)
    print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥: {results_csv_path}")

    # ëª¨ë¸ ì €ì¥
    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    trainer.save_model(save_dir=models_dir, model_name='random_forest')

    # ========================================
    # 10. ìµœì¢… ìš”ì•½
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Š í•™ìŠµ ì™„ë£Œ ìš”ì•½")
    print("=" * 70)

    print(f"\nğŸ¯ ëª¨ë¸ ì„±ëŠ¥:")
    print(f"  - Test Accuracy: {results['test_accuracy']*100:.2f}%")
    print(f"  - Test Precision: {results['test_precision']*100:.2f}%")
    print(f"  - Test Recall: {results['test_recall']*100:.2f}%")
    print(f"  - Test F1-Score: {results['test_f1']:.3f}")

    print(f"\nğŸŒ² ìƒìœ„ 5ê°œ ì¤‘ìš” íŠ¹ì§•:")
    for i, row in feature_imp.head(5).iterrows():
        print(f"  {i+1}. {row['feature']:20s}: {row['importance']:.4f}")

    print(f"\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")

    # ì •í™•ë„ í‰ê°€
    if results['test_accuracy'] >= 0.65:
        print(f"  âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥ (ì •í™•ë„ 65% ì´ìƒ)")
    elif results['test_accuracy'] >= 0.55:
        print(f"  âš ï¸  ë³´í†µ ì„±ëŠ¥ (ì •í™•ë„ 55~65%)")
    else:
        print(f"  âŒ ê°œì„  í•„ìš” (ì •í™•ë„ 55% ë¯¸ë§Œ)")

    # ê³¼ì í•© ì²´í¬
    if 'train_accuracy' in results:
        overfit_gap = results['train_accuracy'] - results['test_accuracy']
        if overfit_gap > 0.15:
            print(f"  âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„± (Train-Test ì°¨ì´: {overfit_gap*100:.1f}%p)")
        else:
            print(f"  âœ… ê³¼ì í•© ì—†ìŒ (Train-Test ì°¨ì´: {overfit_gap*100:.1f}%p)")

    # Feature Importance ì¸ì‚¬ì´íŠ¸
    top_feature = feature_imp.iloc[0]['feature']
    top_importance = feature_imp.iloc[0]['importance']
    print(f"  ğŸ” ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•: {top_feature} ({top_importance:.4f})")

    if top_importance > 0.15:
        print(f"     â†’ ì´ íŠ¹ì§•ì´ 15% ì´ìƒ ì˜í–¥ (ë§¤ìš° ì¤‘ìš”)")
    elif top_importance > 0.10:
        print(f"     â†’ ì´ íŠ¹ì§•ì´ 10% ì´ìƒ ì˜í–¥ (ì¤‘ìš”)")

    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"  - {fi_path}")
    print(f"  - {cm_path}")
    print(f"  - {fi_csv_path}")
    print(f"  - {results_csv_path}")

    print("\n" + "=" * 70)
    print("âœ¨ Day 2 ì™„ë£Œ!")
    print("=" * 70)

    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. âœ… Random Forest í•™ìŠµ ì™„ë£Œ")
    print(f"  2. â­ï¸  ë°±í…ŒìŠ¤íŒ…ìœ¼ë¡œ ì‹¤ì „ ì„±ëŠ¥ ê²€ì¦")
    print(f"  3. â­ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print(f"  4. â­ï¸  ë‹¤ë¥¸ ëª¨ë¸ (XGBoost, LSTM) ì‹œë„")


if __name__ == "__main__":
    main()
