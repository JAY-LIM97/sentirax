"""
Machine Learning Trainer for Sentirax

ğŸ“š ì´ ëª¨ë“ˆì´ í•˜ëŠ” ì¼:
1. ë°ì´í„° ì •ê·œí™” (Normalization/Scaling)
2. Train/Test ë¶„í•  (ì‹œê³„ì—´ íŠ¹ì„± ìœ ì§€)
3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
4. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ
5. ì˜ˆì¸¡ ë° ë°±í…ŒìŠ¤íŒ…

ğŸ“ í•µì‹¬ ê°œë…:
- ì •ê·œí™”: íŠ¹ì§•ë§ˆë‹¤ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ ë¬¸ì œ í•´ê²°
- Train/Test ë¶„í• : ê³¼ì í•© ë°©ì§€, ì‹¤ì „ ì„±ëŠ¥ í‰ê°€
- Random Forest: ì—¬ëŸ¬ Decision Treeì˜ ì•™ìƒë¸”
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, Any
import pickle
import os
from datetime import datetime

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)


class MLTrainer:
    """
    ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë° í‰ê°€ í´ë˜ìŠ¤

    ğŸ¯ ì„¤ê³„ ì² í•™:
    1. ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„± ì¡´ì¤‘ (ë¬´ì‘ìœ„ ì„ì§€ ì•ŠìŒ)
    2. ê³¼ì í•© ë°©ì§€ (Train/Test ì—„ê²© ë¶„ë¦¬)
    3. ì¬í˜„ì„± (random_state ê³ ì •)
    4. í™•ì¥ì„± (ì—¬ëŸ¬ ëª¨ë¸ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥)
    """

    def __init__(self, test_size: float = 0.2, random_state: int = 42):
        """
        Args:
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 20%)
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ

        ğŸ’¡ ì™œ 20%ì¸ê°€?
        - ë„ˆë¬´ ì‘ìœ¼ë©´: í…ŒìŠ¤íŠ¸ ì‹ ë¢°ë„ ë‚®ìŒ
        - ë„ˆë¬´ í¬ë©´: í•™ìŠµ ë°ì´í„° ë¶€ì¡±
        - 20%: ì¼ë°˜ì ìœ¼ë¡œ ê²€ì¦ëœ ë¹„ìœ¨
        """
        self.test_size = test_size
        self.random_state = random_state

        # ë‚˜ì¤‘ì— ì €ì¥/ë¡œë“œí•  ê°ì²´ë“¤
        self.scaler = None
        self.model = None
        self.feature_names = None
        self.training_history = {}

    def normalize_data(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë°ì´í„° ì •ê·œí™” (í‘œì¤€í™”)

        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°

        Returns:
            (ì •ê·œí™”ëœ í•™ìŠµ ë°ì´í„°, ì •ê·œí™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°)

        ğŸ“ ì •ê·œí™”(Normalization)ë€?
        - ëª©ì : ëª¨ë“  íŠ¹ì§•ì„ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶”ê¸°
        - ë°©ë²•: StandardScaler ì‚¬ìš©
          â†’ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜
        - ê³µì‹: (x - mean) / std

        ğŸ’¡ ì™œ í•„ìš”í•œê°€?
        ì˜ˆì‹œ:
        - RSI: 0~100 ë²”ìœ„
        - return_1d: -10~+10 ë²”ìœ„
        - vix: 10~40 ë²”ìœ„

        ì •ê·œí™” ì „:
        - ëª¨ë¸ì´ í° ìˆ«ì(RSI)ì—ë§Œ ì§‘ì¤‘
        - ì‘ì€ ìˆ«ì(return) ë¬´ì‹œë¨

        ì •ê·œí™” í›„:
        - ëª¨ë“  íŠ¹ì§•ì´ -3~+3 ë²”ìœ„
        - ê³µí‰í•˜ê²Œ ê³ ë ¤ë¨

        âš ï¸ ì¤‘ìš”í•œ ê·œì¹™:
        1. Train ë°ì´í„°ë¡œë§Œ fit (í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°)
        2. Test ë°ì´í„°ëŠ” transformë§Œ (Train í†µê³„ ì‚¬ìš©)
        3. ì™œ? TestëŠ” "ë¯¸ë˜ ë°ì´í„°"ë¼ í‰ê· /í‘œì¤€í¸ì°¨ ëª¨ë¦„

        ğŸ“Š ì‹¤ì „ ì˜ˆì‹œ:
        ```
        Train í‰ê· : 50, í‘œì¤€í¸ì°¨: 10
        TestëŠ” ì´ ê°’ ì‚¬ìš©!
        Testì˜ í‰ê· ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ (ì •ë³´ ìœ ì¶œ)
        ```
        """

        print("\nğŸ”§ [Step 1] ë°ì´í„° ì •ê·œí™” (StandardScaler)...")

        # StandardScaler ì´ˆê¸°í™”
        self.scaler = StandardScaler()

        # âš ï¸ Train ë°ì´í„°ë¡œë§Œ fit!
        # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ Trainì—ì„œë§Œ ê³„ì‚°
        self.scaler.fit(X_train)

        # ì •ê·œí™” ì ìš©
        X_train_scaled = self.scaler.transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        print(f"âœ… ì •ê·œí™” ì™„ë£Œ")
        print(f"  - Train í‰ê· : {self.scaler.mean_[:3].round(2)} ... (ì²˜ìŒ 3ê°œ íŠ¹ì§•)")
        print(f"  - Train í‘œì¤€í¸ì°¨: {self.scaler.scale_[:3].round(2)} ... (ì²˜ìŒ 3ê°œ íŠ¹ì§•)")
        print(f"  - ì •ê·œí™” í›„ Train í‰ê· : {X_train_scaled.mean(axis=0)[:3].round(4)} (ê±°ì˜ 0)")
        print(f"  - ì •ê·œí™” í›„ Train í‘œì¤€í¸ì°¨: {X_train_scaled.std(axis=0)[:3].round(4)} (ê±°ì˜ 1)")

        return X_train_scaled, X_test_scaled

    def split_data(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        ì‹œê³„ì—´ Train/Test ë¶„í• 

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: ë¼ë²¨ ë°ì´í„°

        Returns:
            (X_train, X_test, y_train, y_test)

        ğŸ“ ì‹œê³„ì—´ ë¶„í• ì˜ ì¤‘ìš”ì„±:

        âŒ ì¼ë°˜ ë¶„í•  (ë¬´ì‘ìœ„ ì„ê¸°):
        ```
        ì „ì²´: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        Train: [1, 3, 5, 7, 9]  â† ë¬´ì‘ìœ„
        Test: [2, 4, 6, 8, 10]  â† ë¬´ì‘ìœ„

        ë¬¸ì œ: "ë¯¸ë˜"ê°€ "ê³¼ê±°"ë¥¼ í•™ìŠµí•¨ (ì‹œê°„ ì—­í–‰!)
        ```

        âœ… ì‹œê³„ì—´ ë¶„í•  (ìˆœì„œ ìœ ì§€):
        ```
        ì „ì²´: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        Train: [1, 2, 3, 4, 5, 6, 7, 8]  â† ê³¼ê±°
        Test: [9, 10]                    â† ë¯¸ë˜

        ì¥ì : ì‹¤ì „ê³¼ ë™ì¼ (ê³¼ê±°ë¡œ ë¯¸ë˜ ì˜ˆì¸¡)
        ```

        ğŸ’¡ ì™œ ì´ë ‡ê²Œ í•´ì•¼ í•˜ë‚˜?
        - ì‹¤ì „ì—ì„œëŠ” "ë‚´ì¼"ì„ ëª¨ë¥´ëŠ” ìƒíƒœì—ì„œ ì˜ˆì¸¡
        - Trainì— ë¯¸ë˜ ì •ë³´ê°€ ì„ì´ë©´ â†’ ê³¼ì í•©
        - TestëŠ” ë°˜ë“œì‹œ Train ì´í›„ ì‹œì ì´ì–´ì•¼ í•¨

        ğŸ“Š ìš°ë¦¬ í”„ë¡œì íŠ¸:
        - ì´ 77ì¼ ë°ì´í„°
        - Train: ì²˜ìŒ 80% (ì•½ 62ì¼)
        - Test: ë§ˆì§€ë§‰ 20% (ì•½ 15ì¼)
        """

        print("\nğŸ“Š [Step 2] Train/Test ë¶„í•  (ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€)...")

        # ì‹œê³„ì—´ ë¶„í• : ìˆœì„œ ìœ ì§€!
        split_idx = int(len(X) * (1 - self.test_size))

        # ì ˆëŒ€ ì„ì§€ ì•ŠìŒ! (shuffle=False)
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]

        print(f"âœ… ë¶„í•  ì™„ë£Œ")
        print(f"  - ì „ì²´ ë°ì´í„°: {len(X)}ê°œ")
        print(f"  - Train: {len(X_train)}ê°œ ({len(X_train)/len(X)*100:.1f}%)")
        print(f"  - Test: {len(X_test)}ê°œ ({len(X_test)/len(X)*100:.1f}%)")
        print(f"\n  ğŸ’¡ Train ë¼ë²¨ ë¶„í¬:")
        print(f"     - ë§¤ìˆ˜(1): {(y_train == 1).sum()}ê°œ ({(y_train == 1).sum()/len(y_train)*100:.1f}%)")
        print(f"     - ë§¤ë„(0): {(y_train == 0).sum()}ê°œ ({(y_train == 0).sum()/len(y_train)*100:.1f}%)")
        print(f"\n  ğŸ’¡ Test ë¼ë²¨ ë¶„í¬:")
        print(f"     - ë§¤ìˆ˜(1): {(y_test == 1).sum()}ê°œ")
        print(f"     - ë§¤ë„(0): {(y_test == 0).sum()}ê°œ")

        return X_train, X_test, y_train, y_test

    def train_random_forest(
        self,
        X_train: np.ndarray,
        y_train: pd.Series,
        n_estimators: int = 100,
        max_depth: int = 10,
        min_samples_split: int = 5,
        min_samples_leaf: int = 2
    ) -> RandomForestClassifier:
        """
        Random Forest ëª¨ë¸ í•™ìŠµ

        Args:
            X_train: ì •ê·œí™”ëœ í•™ìŠµ ë°ì´í„°
            y_train: í•™ìŠµ ë¼ë²¨
            n_estimators: íŠ¸ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ 100)
            max_depth: íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸ 10)
            min_samples_split: ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 5)
            min_samples_leaf: ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 2)

        Returns:
            í•™ìŠµëœ Random Forest ëª¨ë¸

        ğŸ“ Random Forestë€?

        1. **Decision Tree (ê²°ì • íŠ¸ë¦¬)**:
           ```
           RSI < 30?
           â”œâ”€ Yes â†’ ë§¤ìˆ˜ (ê³¼ë§¤ë„)
           â””â”€ No â†’ VIX > 25?
                   â”œâ”€ Yes â†’ ë§¤ë„ (ê³µí¬)
                   â””â”€ No â†’ ë§¤ìˆ˜
           ```
           - ì¥ì : ì´í•´í•˜ê¸° ì‰¬ì›€
           - ë‹¨ì : ê³¼ì í•© ìœ„í—˜

        2. **Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)**:
           ```
           Tree 1: RSI ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨ â†’ ë§¤ìˆ˜
           Tree 2: VIX ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨ â†’ ë§¤ë„
           Tree 3: Volume ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨ â†’ ë§¤ìˆ˜
           ...
           Tree 100: ì¢…í•© íŒë‹¨ â†’ ë§¤ìˆ˜

           ìµœì¢… ê²°ê³¼: íˆ¬í‘œ (ë§¤ìˆ˜ 60í‘œ vs ë§¤ë„ 40í‘œ) â†’ ë§¤ìˆ˜!
           ```
           - ì¥ì : ê³¼ì í•© ë°©ì§€, ì•ˆì •ì 
           - ë‹¨ì : ëŠë¦¼ (íŠ¸ë¦¬ 100ê°œ)

        ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ëª…:

        - **n_estimators (íŠ¸ë¦¬ ê°œìˆ˜)**: 100
          - ë§ì„ìˆ˜ë¡: ì„±ëŠ¥ â†‘, ëŠë¦¼ â†‘
          - 100~200 ì ë‹¹

        - **max_depth (ìµœëŒ€ ê¹Šì´)**: 10
          - ì‘ì„ìˆ˜ë¡: ê³¼ì í•© ë°©ì§€
          - ë„ˆë¬´ ì‘ìœ¼ë©´: ì„±ëŠ¥ â†“
          - 10~15 ì ë‹¹

        - **min_samples_split**: 5
          - ë…¸ë“œ ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
          - ê³¼ì í•© ë°©ì§€

        - **min_samples_leaf**: 2
          - ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ
          - ê³¼ì í•© ë°©ì§€

        ğŸ¯ ìš°ë¦¬ê°€ Random Forestë¥¼ ì„ íƒí•œ ì´ìœ :
        1. ê³¼ì í•©ì— ê°•í•¨ (ë°ì´í„° 77ê°œë¡œ ì ìŒ)
        2. Feature Importance ì œê³µ (ì–´ë–¤ íŠ¹ì§•ì´ ì¤‘ìš”í•œì§€)
        3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‰¬ì›€
        4. ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜ (Kaggle ë§ì´ ì‚¬ìš©)
        """

        print("\nğŸŒ² [Step 3] Random Forest í•™ìŠµ...")

        # Random Forest ì´ˆê¸°í™”
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            random_state=self.random_state,
            n_jobs=-1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
            class_weight='balanced'  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ì¡°ì •
        )

        print(f"  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        print(f"    - íŠ¸ë¦¬ ê°œìˆ˜ (n_estimators): {n_estimators}")
        print(f"    - ìµœëŒ€ ê¹Šì´ (max_depth): {max_depth}")
        print(f"    - ë¶„í•  ìµœì†Œ ìƒ˜í”Œ (min_samples_split): {min_samples_split}")
        print(f"    - ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ (min_samples_leaf): {min_samples_leaf}")
        print(f"    - Class Weight: balanced (ë¶ˆê· í˜• ìë™ ì¡°ì •)")

        # í•™ìŠµ ì‹œì‘
        print(f"\n  í•™ìŠµ ì‹œì‘...")
        start_time = datetime.now()

        self.model.fit(X_train, y_train)

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… í•™ìŠµ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")

        return self.model

    def evaluate_model(
        self,
        X_test: np.ndarray,
        y_test: pd.Series,
        X_train: np.ndarray = None,
        y_train: pd.Series = None
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

        Args:
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨
            X_train: í•™ìŠµ ë°ì´í„° (ì„ íƒ)
            y_train: í•™ìŠµ ë¼ë²¨ (ì„ íƒ)

        Returns:
            ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬

        ğŸ“ í‰ê°€ ì§€í‘œ ì„¤ëª…:

        1. **Accuracy (ì •í™•ë„)**:
           - ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
           - ê³µì‹: (ë§ì¶˜ ê°œìˆ˜) / (ì „ì²´ ê°œìˆ˜)
           - ì˜ˆ: 10ê°œ ì¤‘ 7ê°œ ë§ì¶¤ â†’ 70%

        2. **Precision (ì •ë°€ë„)**:
           - ë§¤ìˆ˜ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ìƒìŠ¹ ë¹„ìœ¨
           - "ë§¤ìˆ˜ ì‹ í˜¸ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€?"
           - ë†’ì„ìˆ˜ë¡: í—ˆìœ„ ì‹ í˜¸ ì ìŒ

        3. **Recall (ì¬í˜„ìœ¨)**:
           - ì‹¤ì œ ìƒìŠ¹ ì¤‘ ì¡ì•„ë‚¸ ë¹„ìœ¨
           - "ìƒìŠ¹ ê¸°íšŒë¥¼ ì–¼ë§ˆë‚˜ ë†“ì¹˜ì§€ ì•Šì•˜ë‚˜?"
           - ë†’ì„ìˆ˜ë¡: ê¸°íšŒ ë†“ì¹¨ ì ìŒ

        4. **F1-Score**:
           - Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
           - ê· í˜• ì¡íŒ ì„±ëŠ¥ ì§€í‘œ

        5. **Confusion Matrix (í˜¼ë™ í–‰ë ¬)**:
           ```
                      ì˜ˆì¸¡ ë§¤ë„(0)  ì˜ˆì¸¡ ë§¤ìˆ˜(1)
           ì‹¤ì œ ë§¤ë„(0)     TN          FP
           ì‹¤ì œ ë§¤ìˆ˜(1)     FN          TP

           TN (True Negative): ë§¤ë„ ë§ì¶¤
           TP (True Positive): ë§¤ìˆ˜ ë§ì¶¤
           FN (False Negative): ë§¤ìˆ˜ ë†“ì¹¨
           FP (False Positive): í—ˆìœ„ ë§¤ìˆ˜
           ```

        ğŸ’¡ ì£¼ì‹ ë§¤ë§¤ì—ì„œ ì¤‘ìš”í•œ ì§€í‘œ:
        - Precision: ë§¤ìˆ˜ ì‹ í˜¸ì˜ ì‹ ë¢°ë„
        - Recall: ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•ŠëŠ” ëŠ¥ë ¥
        - F1-Score: ì¢…í•© ì„±ëŠ¥
        """

        print("\nğŸ“ˆ [Step 4] ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")

        results = {}

        # ì˜ˆì¸¡
        y_pred_test = self.model.predict(X_test)

        # Test ì„±ëŠ¥
        results['test_accuracy'] = accuracy_score(y_test, y_pred_test)
        results['test_precision'] = precision_score(y_test, y_pred_test, zero_division=0)
        results['test_recall'] = recall_score(y_test, y_pred_test, zero_division=0)
        results['test_f1'] = f1_score(y_test, y_pred_test, zero_division=0)
        results['confusion_matrix'] = confusion_matrix(y_test, y_pred_test)

        print(f"âœ… Test ì„±ëŠ¥:")
        print(f"  - Accuracy (ì •í™•ë„): {results['test_accuracy']*100:.2f}%")
        print(f"  - Precision (ì •ë°€ë„): {results['test_precision']*100:.2f}%")
        print(f"  - Recall (ì¬í˜„ìœ¨): {results['test_recall']*100:.2f}%")
        print(f"  - F1-Score: {results['test_f1']:.3f}")

        print(f"\n  ğŸ“Š Confusion Matrix:")
        cm = results['confusion_matrix']
        print(f"                ì˜ˆì¸¡ ë§¤ë„(0)  ì˜ˆì¸¡ ë§¤ìˆ˜(1)")
        print(f"  ì‹¤ì œ ë§¤ë„(0)      {cm[0,0]:3d}         {cm[0,1]:3d}")
        print(f"  ì‹¤ì œ ë§¤ìˆ˜(1)      {cm[1,0]:3d}         {cm[1,1]:3d}")

        # Train ì„±ëŠ¥ (ê³¼ì í•© í™•ì¸)
        if X_train is not None and y_train is not None:
            y_pred_train = self.model.predict(X_train)
            results['train_accuracy'] = accuracy_score(y_train, y_pred_train)

            print(f"\n  ğŸ’¡ ê³¼ì í•© ì²´í¬:")
            print(f"     - Train Accuracy: {results['train_accuracy']*100:.2f}%")
            print(f"     - Test Accuracy: {results['test_accuracy']*100:.2f}%")
            print(f"     - ì°¨ì´: {(results['train_accuracy'] - results['test_accuracy'])*100:.2f}%p")

            if results['train_accuracy'] - results['test_accuracy'] > 0.15:
                print(f"     âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„± (ì°¨ì´ 15%p ì´ìƒ)")
            else:
                print(f"     âœ… ê³¼ì í•© ì—†ìŒ")

        return results

    def get_feature_importance(self, feature_names: list) -> pd.DataFrame:
        """
        Feature Importance ì¶”ì¶œ

        Args:
            feature_names: íŠ¹ì§• ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            Feature Importance DataFrame

        ğŸ“ Feature Importanceë€?
        - Random Forestê°€ ê° íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©í–ˆëŠ”ì§€
        - 0~1 ì‚¬ì´ ê°’ (í•©ê³„ 1.0)
        - ë†’ì„ìˆ˜ë¡ ì¤‘ìš”

        ğŸ’¡ í™œìš©:
        1. ì¤‘ìš”í•œ íŠ¹ì§• íŒŒì•…
        2. ë¶ˆí•„ìš”í•œ íŠ¹ì§• ì œê±°
        3. ë„ë©”ì¸ ì§€ì‹ ê²€ì¦
           ì˜ˆ: "ê±°ë˜ëŸ‰ì´ ì¤‘ìš”í•˜ë‹¤" â†’ Importance ë†’ìœ¼ë©´ ê²€ì¦ë¨
        """

        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

        importances = self.model.feature_importances_
        feature_imp = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        return feature_imp

    def save_model(self, save_dir: str = 'models', model_name: str = 'random_forest'):
        """
        ëª¨ë¸ ì €ì¥

        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            model_name: ëª¨ë¸ ì´ë¦„
        """

        os.makedirs(save_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{model_name}_{timestamp}.pkl"
        filepath = os.path.join(save_dir, filename)

        # ì €ì¥í•  ê°ì²´
        save_obj = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'training_history': self.training_history
        }

        with open(filepath, 'wb') as f:
            pickle.dump(save_obj, f)

        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_model(self, filepath: str):
        """
        ëª¨ë¸ ë¡œë“œ

        Args:
            filepath: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        """

        with open(filepath, 'rb') as f:
            save_obj = pickle.load(f)

        self.model = save_obj['model']
        self.scaler = save_obj['scaler']
        self.feature_names = save_obj['feature_names']
        self.training_history = save_obj['training_history']

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")


# ========================================
# ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ
# ========================================

"""
ğŸ“ ë¨¸ì‹ ëŸ¬ë‹ í•µì‹¬ ê°œë… ì •ë¦¬

1. **ê³¼ì í•© (Overfitting)**
   - í•™ìŠµ ë°ì´í„°ì—ë§Œ ë§ì¶¤
   - ìƒˆ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ë–¨ì–´ì§
   - ì§•í›„: Train >> Test ì •í™•ë„
   - í•´ê²°: ì •ê·œí™”, ë°ì´í„° ì¦ê°•, ì•™ìƒë¸”

2. **ê³¼ì†Œì í•© (Underfitting)**
   - í•™ìŠµ ìì²´ê°€ ë¶€ì¡±
   - Train, Test ë‘˜ ë‹¤ ë‚®ìŒ
   - í•´ê²°: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€, íŠ¹ì§• ì¶”ê°€

3. **í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„**
   - í¸í–¥(Bias): ë‹¨ìˆœí™” ì˜¤ë¥˜
   - ë¶„ì‚°(Variance): ê³¼ë¯¼ ë°˜ì‘
   - Random Forest: ë¶„ì‚° ë‚®ì¶¤ (ì•™ìƒë¸”)

4. **ì •ê·œí™” vs í‘œì¤€í™”**
   - MinMaxScaler: 0~1 ë²”ìœ„
   - StandardScaler: í‰ê·  0, í‘œì¤€í¸ì°¨ 1
   - ìš°ë¦¬ ì„ íƒ: StandardScaler (ì´ìƒì¹˜ì— ê°•í•¨)

5. **Class Imbalance (í´ë˜ìŠ¤ ë¶ˆê· í˜•)**
   - ë§¤ìˆ˜ 60%, ë§¤ë„ 40% ê°™ì€ ìƒí™©
   - ë¬¸ì œ: ë‹¤ìˆ˜ í´ë˜ìŠ¤ë§Œ ë§ì¶”ë ¤ í•¨
   - í•´ê²°: class_weight='balanced'

6. **êµì°¨ ê²€ì¦ (Cross-Validation)**
   - ì¼ë°˜: K-Fold (ë°ì´í„° Kê°œë¡œ ë¶„í• )
   - ì‹œê³„ì—´: Time Series Split
   - ìš°ë¦¬: ë‹¨ìˆœ Train/Test (ë°ì´í„° ì ìŒ)
"""
